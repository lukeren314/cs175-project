{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS Project 175 Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.22.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (0.29.28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading torchmetrics-0.10.2-py3-none-any.whl (529 kB)\n",
      "     -------------------------------------- 529.7/529.7 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchmetrics) (1.22.3)\n",
      "Requirement already satisfied: torch>=1.3.1 in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchmetrics) (1.13.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.3.1->torchmetrics) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\luker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->torchmetrics) (3.0.7)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model \n",
    "from sklearn import metrics \n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "def gather_data():\n",
    "    data = pd.read_csv('./data/data.csv')\n",
    "    return data['lyrics'], data['genre']\n",
    "\n",
    "def vectorize_labels(labels, classes=None):\n",
    "    '''\n",
    "    Vectorizes the labels.\n",
    "    Returns as (indexes, labels)\n",
    "    '''\n",
    "    if classes is None:\n",
    "        return pd.factorize(labels)\n",
    "    return pd.Categorical(labels, categories=classes).codes, classes\n",
    "\n",
    "\n",
    "# PHASE 1 START ----------------------------------------------------------------\n",
    "def features_bow(data):\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df=0.01, ngram_range=(1, 2))\n",
    "    text = data.to_list()\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X, vectorizer\n",
    "\n",
    "def train_model_logistic(X, Y):\n",
    "    classifier = linear_model.LogisticRegression(penalty='l2', multi_class='multinomial', class_weight='balanced', random_state=RANDOM_STATE, fit_intercept=True)\n",
    "    classifier.fit(X, Y)\n",
    "    return classifier\n",
    "\n",
    "def evaluate_model_sklearn(model, X_train, Y_train, X_test, Y_test):\n",
    "    train_accuracy = model.score(X_train, Y_train)\n",
    "    print('\\nTraining:')\n",
    "    print(' accuracy:',format( 100*train_accuracy , '.2f') ) \n",
    "\n",
    "    # Compute and print accuracy on the test data\n",
    "    print('\\nTesting: ')\n",
    "    test_accuracy = model.score(X_test, Y_test)\n",
    "    print(' accuracy:', format( 100*test_accuracy , '.2f') )\n",
    "\n",
    "    # Compute and print AUC on the test data\n",
    "    class_probabilities = model.predict_proba(X_test)\n",
    "    test_auc_score = metrics.roc_auc_score(Y_test, class_probabilities, multi_class='ovo')\n",
    "    print(' AUC value:', format( 100*test_auc_score , '.2f') )\n",
    "\n",
    "def sample_incorrect_predictions(predictions, probabilities, actuals, classes, titles, lyrics):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    NUM_EXAMPLES = 10\n",
    "    for _ in range(NUM_EXAMPLES):\n",
    "        i = np.random.choice(np.where(predictions != actuals)[0])\n",
    "        print(\"Song Title:\", titles[i])\n",
    "        print('Predicted:', classes[predictions[i]], 'Actual:', classes[actuals[i]])\n",
    "        print('Probability:', probabilities[i][predictions[i]])\n",
    "        print(\"Lyrics: \")\n",
    "        print('\"' + lyrics[i][:100] + '...\"')\n",
    "        print()\n",
    "\n",
    "# PHASE 1 END ------------------------------------------------------------------\n",
    "\n",
    "# PHASE 2 START ----------------------------------------------------------------\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_model_mlp(X, Y, hidden_layer_sizes=(100,), max_iter=200):\n",
    "    classifier = MLPClassifier(solver='lbfgs', hidden_layer_sizes=hidden_layer_sizes, max_iter=max_iter, random_state=RANDOM_STATE)\n",
    "    classifier.fit(X, Y)\n",
    "    return classifier\n",
    "    \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n",
    "def features_word2vec(data):\n",
    "    word_lists = data.str.split()\n",
    "    if not os.path.exists(\"word2vec.wordvectors\"):\n",
    "        print(\"Did not find pre-trained embeddings, training now...\")\n",
    "        model = Word2Vec(sentences=word_lists, vector_size=200, window=5, min_count=1, workers=4)\n",
    "        word_vectors = model.wv\n",
    "        word_vectors.save(\"word2vec.wordvectors\")\n",
    "    else:\n",
    "        print(\"Found pre-trained embeddings, loading now...\")\n",
    "        word_vectors = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
    "    print(\"Vectorizing features...\")\n",
    "    res = []\n",
    "    i = 0\n",
    "    for word_list in word_lists:\n",
    "        sub = []\n",
    "        for word in word_list:\n",
    "            if word in word_vectors:\n",
    "                sub.append(word_vectors[word])\n",
    "        res.append(sub)\n",
    "        if i % 1000 == 0:\n",
    "            print(i, \"/\", len(word_lists))\n",
    "        i += 1\n",
    "    return res\n",
    "\n",
    "def clean_vectors(X, Y):\n",
    "    # remove inputs with too few vectors\n",
    "    small_inputs = [i for i, x in enumerate(X) if len(x) < 10]\n",
    "    small_inputs_set = set(small_inputs)\n",
    "    X = [v for i, v in enumerate(X) if i not in small_inputs_set]\n",
    "    Y = np.delete(Y, small_inputs)\n",
    "    return X, Y\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import R2Score\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "    \n",
    "# RNN code inspired from HW2\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # self.hidden_layers = [nn.Linear(input_size + hidden_size, hidden_size) for _ in range(n_layers)]\n",
    "        self.hidden_layer =  nn.Linear(input_size + hidden_size, hidden_size) # create hidden layer\n",
    "        self.output_layer =  nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # finish with a log softmax\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "        # Put the computation for forward pass here\n",
    "        combined = torch.cat((input_, hidden), 1) # concatenate the input and hidden layers\n",
    "        output = self.output_layer(combined) # compute the output\n",
    "        output = self.softmax(output) # apply softmax\n",
    "        hidden = self.hidden_layer(combined) # compute the hidden layer\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probabilities = torch.zeros((len(X), 7))\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        i = 0\n",
    "        for song in X:\n",
    "            hidden = self.init_hidden()\n",
    "            song_tensor = torch.tensor(np.expand_dims(song, axis=1), dtype=torch.float)\n",
    "            for i in range(song_tensor.size()[0]):\n",
    "                output, hidden = self.forward(song_tensor[i], hidden)\n",
    "            probabilities[i] = softmax(output)\n",
    "            if i % 1000 == 0:\n",
    "                print(i, \"/\", len(X))\n",
    "            i += 1\n",
    "        return probabilities\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        u = 0\n",
    "        v = 0\n",
    "        i = 0\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        predictions = torch.zeros((len(Y), 7))\n",
    "        actuals = torch.zeroes((len(Y), 7))\n",
    "\n",
    "        for song, genre in zip(X, Y):\n",
    "            hidden = self.init_hidden()\n",
    "            song_tensor = torch.tensor(np.expand_dims(song, axis=1), dtype=torch.float)\n",
    "            for i in range(song_tensor.size()[0]):\n",
    "                output, hidden = self.forward(song_tensor[i], hidden)\n",
    "            one_hot = torch.nn.functional.one_hot(torch.tensor(genre, dtype=torch.long), num_classes=7)\n",
    "            predictions[i] = softmax(output)\n",
    "            actuals[i] = one_hot\n",
    "            if i % 1000 == 0:\n",
    "                print(i, \"/\", len(X))\n",
    "        r2 = R2Score()\n",
    "        return r2(predictions, actuals)\n",
    "\n",
    "def train_model_rnn(X, Y):\n",
    "    model = RNN(input_size=len(X[0][0]), output_size=7, hidden_size=100, n_layers=2)\n",
    "\n",
    "    n_iters = 2\n",
    "    lr=1e-4\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # reshape X\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden()\n",
    "        r_i = np.random.randint(0, len(X))\n",
    "        song_tensor = torch.tensor(np.expand_dims(X[r_i], axis=1), dtype=torch.float)\n",
    "        genre_tensor = torch.tensor(np.expand_dims(Y[r_i], axis=0), dtype=torch.long)\n",
    "        for i in range(song_tensor.size()[0]):\n",
    "            output, hidden = model.forward(song_tensor[i], hidden)\n",
    "        loss = criterion(output, genre_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iter % 1000 == 0:\n",
    "            print(f'{iter}/{n_iters}')\n",
    "    return model\n",
    "\n",
    "def evaluate_model_rnn(model, X_train, Y_train, X_test, Y_test):\n",
    "    train_accuracy = model.score(X_train, Y_train)\n",
    "    print('\\nTraining:')\n",
    "    print(' accuracy:',format( 100*train_accuracy , '.2f') ) \n",
    "\n",
    "    # Compute and print accuracy on the test data\n",
    "    print('\\nTesting: ')\n",
    "    test_accuracy = model.score(X_test, Y_test)\n",
    "    print(' accuracy:', format( 100*test_accuracy , '.2f') )\n",
    "\n",
    "# PHASE 2 END ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP on BOW\n",
    "\n",
    "Let's start with the MLP classifier on BOW, then we'll try RNN on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 pipeline\n",
    "inputs, labels = gather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to classes\n",
    "Y, classes = vectorize_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try MLP on BOW first\n",
    "X_bow, vectorizer = features_bow(inputs)\n",
    "X_bow_train, X_bow_test, Y_bow_train, Y_bow_test = train_test_split(X_bow, Y, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_bow_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luker\\Documents\\School\\2022-2023\\fall2022\\CS175\\cs175-project\\phase2.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luker/Documents/School/2022-2023/fall2022/CS175/cs175-project/phase2.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_nn \u001b[39m=\u001b[39m train_model_mlp(X_bow_train, Y_bow_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_bow_train' is not defined"
     ]
    }
   ],
   "source": [
    "model_nn = train_model_mlp(X_bow_train, Y_bow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:\n",
      " accuracy: 72.66\n",
      "\n",
      "Testing: \n",
      " accuracy: 59.18\n",
      " AUC value: 82.20\n"
     ]
    }
   ],
   "source": [
    "# evaluate MLP on bow\n",
    "evaluate_model_sklearn(model_nn, X_bow_train, Y_bow_train, X_bow_test, Y_bow_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN on BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:\n",
      " accuracy: 84.28\n",
      "\n",
      "Testing: \n",
      " accuracy: 60.76\n",
      " AUC value: 81.35\n"
     ]
    }
   ],
   "source": [
    "# 60.18 (100, 50, 50, 50, 50) \n",
    "# 60.21 (100, 100, 100, 100, 100) 3x longer to train\n",
    "# 79.61 60.32 (200, 200) 21m 49.5s\n",
    "# 76.09 60.66 (200, 100, 100, 100) 69m 21.4s\n",
    "# 84.28 60.76 (500, 100, 100, 100) 183m 23.2s\n",
    "# 89.32 61 (750, 250) 48m (on 6-core)\n",
    "# 89.76 61.85 82.00 (1000, 500) 57 min (on 6-core)\n",
    "model_dnn = train_model_mlp(X_bow_train, Y_bow_train, hidden_layer_sizes=(500, 100, 100, 100))\n",
    "evaluate_model_sklearn(model_dnn, X_bow_train, Y_bow_train, X_bow_test, Y_bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'hidden_layer_sizes':[(500, 500), (500, 250, 250)]\n",
    "}\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=RANDOM_STATE)\n",
    "clf = GridSearchCV(mlp, parameters, n_jobs=4)\n",
    "clf.fit(X_bow, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN on Embeddings\n",
    "\n",
    "Now we're gonna try RNN on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pre-trained embeddings, loading now...\n",
      "Vectorizing features...\n",
      "0 / 295288\n",
      "1000 / 295288\n",
      "2000 / 295288\n",
      "3000 / 295288\n",
      "4000 / 295288\n",
      "5000 / 295288\n",
      "6000 / 295288\n",
      "7000 / 295288\n",
      "8000 / 295288\n",
      "9000 / 295288\n",
      "10000 / 295288\n",
      "11000 / 295288\n",
      "12000 / 295288\n",
      "13000 / 295288\n",
      "14000 / 295288\n",
      "15000 / 295288\n",
      "16000 / 295288\n",
      "17000 / 295288\n",
      "18000 / 295288\n",
      "19000 / 295288\n",
      "20000 / 295288\n",
      "21000 / 295288\n",
      "22000 / 295288\n",
      "23000 / 295288\n",
      "24000 / 295288\n",
      "25000 / 295288\n",
      "26000 / 295288\n",
      "27000 / 295288\n",
      "28000 / 295288\n",
      "29000 / 295288\n",
      "30000 / 295288\n",
      "31000 / 295288\n",
      "32000 / 295288\n",
      "33000 / 295288\n",
      "34000 / 295288\n",
      "35000 / 295288\n",
      "36000 / 295288\n",
      "37000 / 295288\n",
      "38000 / 295288\n",
      "39000 / 295288\n",
      "40000 / 295288\n",
      "41000 / 295288\n",
      "42000 / 295288\n",
      "43000 / 295288\n",
      "44000 / 295288\n",
      "45000 / 295288\n",
      "46000 / 295288\n",
      "47000 / 295288\n",
      "48000 / 295288\n",
      "49000 / 295288\n",
      "50000 / 295288\n",
      "51000 / 295288\n",
      "52000 / 295288\n",
      "53000 / 295288\n",
      "54000 / 295288\n",
      "55000 / 295288\n",
      "56000 / 295288\n",
      "57000 / 295288\n",
      "58000 / 295288\n",
      "59000 / 295288\n",
      "60000 / 295288\n",
      "61000 / 295288\n",
      "62000 / 295288\n",
      "63000 / 295288\n",
      "64000 / 295288\n",
      "65000 / 295288\n",
      "66000 / 295288\n",
      "67000 / 295288\n",
      "68000 / 295288\n",
      "69000 / 295288\n",
      "70000 / 295288\n",
      "71000 / 295288\n",
      "72000 / 295288\n",
      "73000 / 295288\n",
      "74000 / 295288\n",
      "75000 / 295288\n",
      "76000 / 295288\n",
      "77000 / 295288\n",
      "78000 / 295288\n",
      "79000 / 295288\n",
      "80000 / 295288\n",
      "81000 / 295288\n",
      "82000 / 295288\n",
      "83000 / 295288\n",
      "84000 / 295288\n",
      "85000 / 295288\n",
      "86000 / 295288\n",
      "87000 / 295288\n",
      "88000 / 295288\n",
      "89000 / 295288\n",
      "90000 / 295288\n",
      "91000 / 295288\n",
      "92000 / 295288\n",
      "93000 / 295288\n",
      "94000 / 295288\n",
      "95000 / 295288\n",
      "96000 / 295288\n",
      "97000 / 295288\n",
      "98000 / 295288\n",
      "99000 / 295288\n",
      "100000 / 295288\n",
      "101000 / 295288\n",
      "102000 / 295288\n",
      "103000 / 295288\n",
      "104000 / 295288\n",
      "105000 / 295288\n",
      "106000 / 295288\n",
      "107000 / 295288\n",
      "108000 / 295288\n",
      "109000 / 295288\n",
      "110000 / 295288\n",
      "111000 / 295288\n",
      "112000 / 295288\n",
      "113000 / 295288\n",
      "114000 / 295288\n",
      "115000 / 295288\n",
      "116000 / 295288\n",
      "117000 / 295288\n",
      "118000 / 295288\n",
      "119000 / 295288\n",
      "120000 / 295288\n",
      "121000 / 295288\n",
      "122000 / 295288\n",
      "123000 / 295288\n",
      "124000 / 295288\n",
      "125000 / 295288\n",
      "126000 / 295288\n",
      "127000 / 295288\n",
      "128000 / 295288\n",
      "129000 / 295288\n",
      "130000 / 295288\n",
      "131000 / 295288\n",
      "132000 / 295288\n",
      "133000 / 295288\n",
      "134000 / 295288\n",
      "135000 / 295288\n",
      "136000 / 295288\n",
      "137000 / 295288\n",
      "138000 / 295288\n",
      "139000 / 295288\n",
      "140000 / 295288\n",
      "141000 / 295288\n",
      "142000 / 295288\n",
      "143000 / 295288\n",
      "144000 / 295288\n",
      "145000 / 295288\n",
      "146000 / 295288\n",
      "147000 / 295288\n",
      "148000 / 295288\n",
      "149000 / 295288\n",
      "150000 / 295288\n",
      "151000 / 295288\n",
      "152000 / 295288\n",
      "153000 / 295288\n",
      "154000 / 295288\n",
      "155000 / 295288\n",
      "156000 / 295288\n",
      "157000 / 295288\n",
      "158000 / 295288\n",
      "159000 / 295288\n",
      "160000 / 295288\n",
      "161000 / 295288\n",
      "162000 / 295288\n",
      "163000 / 295288\n",
      "164000 / 295288\n",
      "165000 / 295288\n",
      "166000 / 295288\n",
      "167000 / 295288\n",
      "168000 / 295288\n",
      "169000 / 295288\n",
      "170000 / 295288\n",
      "171000 / 295288\n",
      "172000 / 295288\n",
      "173000 / 295288\n",
      "174000 / 295288\n",
      "175000 / 295288\n",
      "176000 / 295288\n",
      "177000 / 295288\n",
      "178000 / 295288\n",
      "179000 / 295288\n",
      "180000 / 295288\n",
      "181000 / 295288\n",
      "182000 / 295288\n",
      "183000 / 295288\n",
      "184000 / 295288\n",
      "185000 / 295288\n",
      "186000 / 295288\n",
      "187000 / 295288\n",
      "188000 / 295288\n",
      "189000 / 295288\n",
      "190000 / 295288\n",
      "191000 / 295288\n",
      "192000 / 295288\n",
      "193000 / 295288\n",
      "194000 / 295288\n",
      "195000 / 295288\n",
      "196000 / 295288\n",
      "197000 / 295288\n",
      "198000 / 295288\n",
      "199000 / 295288\n",
      "200000 / 295288\n",
      "201000 / 295288\n",
      "202000 / 295288\n",
      "203000 / 295288\n",
      "204000 / 295288\n",
      "205000 / 295288\n",
      "206000 / 295288\n",
      "207000 / 295288\n",
      "208000 / 295288\n",
      "209000 / 295288\n",
      "210000 / 295288\n",
      "211000 / 295288\n",
      "212000 / 295288\n",
      "213000 / 295288\n",
      "214000 / 295288\n",
      "215000 / 295288\n",
      "216000 / 295288\n",
      "217000 / 295288\n",
      "218000 / 295288\n",
      "219000 / 295288\n",
      "220000 / 295288\n",
      "221000 / 295288\n",
      "222000 / 295288\n",
      "223000 / 295288\n",
      "224000 / 295288\n",
      "225000 / 295288\n",
      "226000 / 295288\n",
      "227000 / 295288\n",
      "228000 / 295288\n",
      "229000 / 295288\n",
      "230000 / 295288\n",
      "231000 / 295288\n",
      "232000 / 295288\n",
      "233000 / 295288\n",
      "234000 / 295288\n",
      "235000 / 295288\n",
      "236000 / 295288\n",
      "237000 / 295288\n",
      "238000 / 295288\n",
      "239000 / 295288\n",
      "240000 / 295288\n",
      "241000 / 295288\n",
      "242000 / 295288\n",
      "243000 / 295288\n",
      "244000 / 295288\n",
      "245000 / 295288\n",
      "246000 / 295288\n",
      "247000 / 295288\n",
      "248000 / 295288\n",
      "249000 / 295288\n",
      "250000 / 295288\n",
      "251000 / 295288\n",
      "252000 / 295288\n",
      "253000 / 295288\n",
      "254000 / 295288\n",
      "255000 / 295288\n",
      "256000 / 295288\n",
      "257000 / 295288\n",
      "258000 / 295288\n",
      "259000 / 295288\n",
      "260000 / 295288\n",
      "261000 / 295288\n",
      "262000 / 295288\n",
      "263000 / 295288\n",
      "264000 / 295288\n",
      "265000 / 295288\n",
      "266000 / 295288\n",
      "267000 / 295288\n",
      "268000 / 295288\n",
      "269000 / 295288\n",
      "270000 / 295288\n",
      "271000 / 295288\n",
      "272000 / 295288\n",
      "273000 / 295288\n",
      "274000 / 295288\n",
      "275000 / 295288\n",
      "276000 / 295288\n",
      "277000 / 295288\n",
      "278000 / 295288\n",
      "279000 / 295288\n",
      "280000 / 295288\n",
      "281000 / 295288\n",
      "282000 / 295288\n",
      "283000 / 295288\n",
      "284000 / 295288\n",
      "285000 / 295288\n",
      "286000 / 295288\n",
      "287000 / 295288\n",
      "288000 / 295288\n",
      "289000 / 295288\n",
      "290000 / 295288\n",
      "291000 / 295288\n",
      "292000 / 295288\n",
      "293000 / 295288\n",
      "294000 / 295288\n",
      "295000 / 295288\n"
     ]
    }
   ],
   "source": [
    "# takes a long time to run!\n",
    "X_raw = features_word2vec(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec, Y_vec = clean_vectors(X_raw, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test\n",
    "X_vec_train, X_vec_test, Y_vec_train, Y_vec_test = train_test_split(X_vec, Y_vec, test_size = 0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_vec_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luker\\Documents\\School\\2022-2023\\fall2022\\CS175\\cs175-project\\phase2.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luker/Documents/School/2022-2023/fall2022/CS175/cs175-project/phase2.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luker/Documents/School/2022-2023/fall2022/CS175/cs175-project/phase2.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_rnn \u001b[39m=\u001b[39m train_model_rnn(X_vec_train, Y_vec_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_vec_train' is not defined"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model_rnn = train_model_rnn(X_vec_train, Y_vec_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luker\\Documents\\School\\2022-2023\\fall2022\\CS175\\cs175-project\\phase2.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luker/Documents/School/2022-2023/fall2022/CS175/cs175-project/phase2.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate_model_sklearn(model_rnn, X_vec_train, Y_vec_train, X_vec_test, Y_vec_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_rnn' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_model_sklearn(model_rnn, X_vec_train, Y_vec_train, X_vec_test, Y_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the results of Logistic Regression on BOW, Neural Network on BOW, Deep Neural Neural Network on BOW, and RNN on Word Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b2bee90d55464e0a4d3dee92f2ce936a28fa0d3744538f61c264101b3bf2266"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
