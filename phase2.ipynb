{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS Project 175 Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (1.23.4)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (6.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: torch in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (4.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.9\" in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchmetrics) (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchmetrics) (1.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: torch>=1.3.1 in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchmetrics) (1.13.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging->torchmetrics) (3.0.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\lr\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install torch\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model \n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "def gather_data():\n",
    "    data = pd.read_csv('./data/data.csv')\n",
    "    return data['lyrics'], data['genre']\n",
    "\n",
    "def vectorize_labels(labels, classes=None):\n",
    "    '''\n",
    "    Vectorizes the labels.\n",
    "    Returns as (indexes, labels)\n",
    "    '''\n",
    "    if classes is None:\n",
    "        return pd.factorize(labels)\n",
    "    return pd.Categorical(labels, categories=classes).codes, classes\n",
    "\n",
    "\n",
    "# PHASE 1 START ----------------------------------------------------------------\n",
    "def features_bow(data):\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df=0.01, ngram_range=(1, 2))\n",
    "    text = data.to_list()\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X, vectorizer\n",
    "\n",
    "def train_model_logistic(X, Y):\n",
    "    classifier = linear_model.LogisticRegression(penalty='l2', multi_class='multinomial', class_weight='balanced', random_state=RANDOM_STATE, fit_intercept=True)\n",
    "    classifier.fit(X, Y)\n",
    "    return classifier\n",
    "\n",
    "def evaluate_model_sklearn(model, X_train, Y_train, X_test, Y_test):\n",
    "    train_accuracy = model.score(X_train, Y_train)\n",
    "    print('\\nTraining:')\n",
    "    print(' accuracy:',format( 100*train_accuracy , '.2f') ) \n",
    "\n",
    "    # Compute and print accuracy on the test data\n",
    "    print('\\nTesting: ')\n",
    "    test_accuracy = model.score(X_test, Y_test)\n",
    "    print(' accuracy:', format( 100*test_accuracy , '.2f') )\n",
    "\n",
    "    # Compute and print AUC on the test data\n",
    "    class_probabilities = model.predict_proba(X_test)\n",
    "    test_auc_score = metrics.roc_auc_score(Y_test, class_probabilities, multi_class='ovo')\n",
    "    print(' AUC value:', format( 100*test_auc_score , '.2f') )\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "def sample_incorrect_predictions(predictions, probabilities, actuals, classes, titles, lyrics):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    NUM_EXAMPLES = 10\n",
    "    for _ in range(NUM_EXAMPLES):\n",
    "        i = np.random.choice(np.where(predictions != actuals)[0])\n",
    "        print(\"Song Title:\", titles[i])\n",
    "        print('Predicted:', classes[predictions[i]], 'Actual:', classes[actuals[i]])\n",
    "        print('Probability:', probabilities[i][predictions[i]])\n",
    "        print(\"Lyrics: \")\n",
    "        print('\"' + lyrics[i][:100] + '...\"')\n",
    "        print()\n",
    "\n",
    "# PHASE 1 END ------------------------------------------------------------------\n",
    "\n",
    "# Phase 2 separted into different blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP on BOW\n",
    "\n",
    "Let's start with the MLP classifier on BOW, then we'll try RNN on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 pipeline\n",
    "inputs, labels = gather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to classes\n",
    "Y, classes = vectorize_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try MLP on BOW first\n",
    "X_bow, vectorizer = features_bow(inputs)\n",
    "X_bow_train, X_bow_test, Y_bow_train, Y_bow_test = train_test_split(X_bow, Y, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_model_mlp(X, Y, solver='lbfgs', **kwargs):\n",
    "    classifier = MLPClassifier(solver=solver, random_state=RANDOM_STATE, **kwargs)\n",
    "    classifier.fit(X, Y)\n",
    "    return classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luker\\Documents\\School\\2022-2023\\fall2022\\CS175\\cs175-project\\phase2.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luker/Documents/School/2022-2023/fall2022/CS175/cs175-project/phase2.ipynb#Y130sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     classifier\u001b[39m.\u001b[39mfit(X, Y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luker/Documents/School/2022-2023/fall2022/CS175/cs175-project/phase2.ipynb#Y130sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m classifier\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luker/Documents/School/2022-2023/fall2022/CS175/cs175-project/phase2.ipynb#Y130sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model_nn \u001b[39m=\u001b[39m train_model_mlp(X_bow_train, Y_bow_train)\n",
      "\u001b[1;32mc:\\Users\\luker\\Documents\\School\\2022-2023\\fall2022\\CS175\\cs175-project\\phase2.ipynb Cell 12\u001b[0m in \u001b[0;36mtrain_model_mlp\u001b[1;34m(X, Y, solver, **kwargs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luker/Documents/School/2022-2023/fall2022/CS175/cs175-project/phase2.ipynb#Y130sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model_mlp\u001b[39m(X, Y, solver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luker/Documents/School/2022-2023/fall2022/CS175/cs175-project/phase2.ipynb#Y130sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     classifier \u001b[39m=\u001b[39m MLPClassifier(solver\u001b[39m=\u001b[39msolver, random_state\u001b[39m=\u001b[39mRANDOM_STATE, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luker/Documents/School/2022-2023/fall2022/CS175/cs175-project/phase2.ipynb#Y130sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     classifier\u001b[39m.\u001b[39;49mfit(X, Y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luker/Documents/School/2022-2023/fall2022/CS175/cs175-project/phase2.ipynb#Y130sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m classifier\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:762\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[0;32m    746\u001b[0m     \u001b[39m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \n\u001b[0;32m    748\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[39m        Returns a trained MLP model.\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, incremental\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:441\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m# Run the LBFGS solver\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_lbfgs(\n\u001b[0;32m    442\u001b[0m         X, y, activations, deltas, coef_grads, intercept_grads, layer_units\n\u001b[0;32m    443\u001b[0m     )\n\u001b[0;32m    445\u001b[0m \u001b[39m# validate parameter weights\u001b[39;00m\n\u001b[0;32m    446\u001b[0m weights \u001b[39m=\u001b[39m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoefs_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercepts_)\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit_lbfgs\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    544\u001b[0m     iprint \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m--> 546\u001b[0m opt_res \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49moptimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[0;32m    547\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loss_grad_lbfgs,\n\u001b[0;32m    548\u001b[0m     packed_coef_inter,\n\u001b[0;32m    549\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    550\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    551\u001b[0m     options\u001b[39m=\u001b[39;49m{\n\u001b[0;32m    552\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmaxfun\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_fun,\n\u001b[0;32m    553\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    554\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint,\n\u001b[0;32m    555\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m    556\u001b[0m     },\n\u001b[0;32m    557\u001b[0m     args\u001b[39m=\u001b[39;49m(X, y, activations, deltas, coef_grads, intercept_grads),\n\u001b[0;32m    558\u001b[0m )\n\u001b[0;32m    559\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m _check_optimize_result(\u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m, opt_res, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter)\n\u001b[0;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_ \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:681\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    679\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    680\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 681\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    682\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    683\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    684\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    685\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:362\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    356\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    357\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    360\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    363\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    364\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m     \u001b[39m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:235\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._loss_grad_lbfgs\u001b[1;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39m\"\"\"Compute the MLP loss function and its corresponding derivatives\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39mwith respect to the different parameters given in the initialization.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mgrad : array-like, shape (number of nodes of all layers,)\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unpack(packed_coef_inter)\n\u001b[1;32m--> 235\u001b[0m loss, coef_grads, intercept_grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backprop(\n\u001b[0;32m    236\u001b[0m     X, y, activations, deltas, coef_grads, intercept_grads\n\u001b[0;32m    237\u001b[0m )\n\u001b[0;32m    238\u001b[0m grad \u001b[39m=\u001b[39m _pack(coef_grads, intercept_grads)\n\u001b[0;32m    239\u001b[0m \u001b[39mreturn\u001b[39;00m loss, grad\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:280\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._backprop\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    277\u001b[0m n_samples \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    279\u001b[0m \u001b[39m# Forward propagate\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m activations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_pass(activations)\n\u001b[0;32m    282\u001b[0m \u001b[39m# Get loss\u001b[39;00m\n\u001b[0;32m    283\u001b[0m loss_func_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:131\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._forward_pass\u001b[1;34m(self, activations)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m# Iterate over the hidden layers\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_layers_ \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m--> 131\u001b[0m     activations[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m safe_sparse_dot(activations[i], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoefs_[i])\n\u001b[0;32m    132\u001b[0m     activations[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercepts_[i]\n\u001b[0;32m    134\u001b[0m     \u001b[39m# For the hidden layers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\extmath.py:152\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    150\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[0;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    155\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[0;32m    156\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[0;32m    157\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[0;32m    158\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    159\u001b[0m ):\n\u001b[0;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:620\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    618\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    619\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:525\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_vector(other\u001b[39m.\u001b[39mravel())\u001b[39m.\u001b[39mreshape(M, \u001b[39m1\u001b[39m)\n\u001b[0;32m    524\u001b[0m     \u001b[39melif\u001b[39;00m other\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m N:\n\u001b[1;32m--> 525\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_multivector(other)\n\u001b[0;32m    527\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    528\u001b[0m     \u001b[39m# scalar value\u001b[39;00m\n\u001b[0;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_scalar(other)\n",
      "File \u001b[1;32mc:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py:502\u001b[0m, in \u001b[0;36m_cs_matrix._mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[39m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[0;32m    501\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_sparsetools, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_matvecs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 502\u001b[0m fn(M, N, n_vecs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m    503\u001b[0m    other\u001b[39m.\u001b[39;49mravel(), result\u001b[39m.\u001b[39;49mravel())\n\u001b[0;32m    505\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_nn = train_model_mlp(X_bow_train, Y_bow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate MLP on bow\n",
    "evaluate_model_sklearn(model_nn, X_bow_train, Y_bow_train, X_bow_test, Y_bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:\n",
      " accuracy: 72.66\n",
      "\n",
      "Testing: \n",
      " accuracy: 59.18\n",
      " AUC value: 82.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:\n",
      " accuracy: 77.01\n",
      "\n",
      "Testing: \n",
      " accuracy: 59.23\n",
      " AUC value: 81.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luker\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:\n",
      " accuracy: 86.51\n",
      "\n",
      "Testing: \n",
      " accuracy: 57.73\n",
      " AUC value: 79.33\n"
     ]
    }
   ],
   "source": [
    "# axis search\n",
    "table = {}\n",
    "params = ['lbfgs', 'sgd', 'adam']\n",
    "for param in params:\n",
    "    model = train_model_mlp(X_bow_train, Y_bow_train, solver=param)\n",
    "    train, test = evaluate_model_sklearn(model, X_bow_train, Y_bow_train, X_bow_test, Y_bow_test)\n",
    "    table[param] = (train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN on BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60.18 (100, 50, 50, 50, 50) \n",
    "# 60.21 (100, 100, 100, 100, 100) 3x longer to train\n",
    "# 79.61 60.32 (200, 200) 21m 49.5s\n",
    "# 76.09 60.66 (200, 100, 100, 100) 69m 21.4s\n",
    "# 84.28 60.76 (500, 100, 100, 100) 183m 23.2s\n",
    "# 89.32 61 (750, 250) 48m (on 6-core)\n",
    "# 89.76 61.85 82.00 (1000, 500) 57 min (on 6-core)\n",
    "model_dnn = train_model_mlp(X_bow_train, Y_bow_train, hidden_layer_sizes=(500, 100, 100, 100))\n",
    "evaluate_model_sklearn(model_dnn, X_bow_train, Y_bow_train, X_bow_test, Y_bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {}\n",
    "params = [(500, 500, 500), (1000, 1000), (250, 250, 250, 250), (1000, 500, 250)]\n",
    "for param in params:\n",
    "    model = train_model_mlp(X_bow_train, Y_bow_train, solver='sgd', hidden_layer_sizes=param)\n",
    "    train, test = evaluate_model_sklearn(model, X_bow_train, Y_bow_train, X_bow_test, Y_bow_test)\n",
    "    table[param] = (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN on Embeddings\n",
    "\n",
    "Now we're gonna try RNN on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str = inputs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pre-trained embeddings, loading now...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"word2vec.wordvectors\"):\n",
    "    print(\"Did not find pre-trained embeddings, training now...\")\n",
    "    model = Word2Vec(sentences=inputs.str.split(), vector_size=100, window=5, min_count=1, workers=4)\n",
    "    word_vectors = model.wv\n",
    "    word_vectors.save(\"word2vec.wordvectors\")\n",
    "else:\n",
    "    print(\"Found pre-trained embeddings, loading now...\")\n",
    "    word_vectors = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test\n",
    "X_vec_train, X_vec_test, Y_vec_train, Y_vec_test = train_test_split(X_str, Y, test_size = 0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word2vec(tokens):\n",
    "    return np.array([word_vectors[word] for word in tokens if word in word_vectors])\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import R2Score\n",
    "import torch.multiprocessing as mp\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "import gc\n",
    "\n",
    "# RNN code inspired from HW2\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # self.hidden_layers = [nn.Linear(input_size + hidden_size, hidden_size) for _ in range(n_layers)]\n",
    "        self.hidden_layer =  nn.Linear(input_size + hidden_size, hidden_size) # create hidden layer\n",
    "        self.output_layer =  nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # finish with a log softmax\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "        # Put the computation for forward pass here\n",
    "        combined = torch.cat((input_, hidden), 1) # concatenate the input and hidden layers\n",
    "        output = self.output_layer(combined) # compute the output\n",
    "        output = self.softmax(output) # apply softmax\n",
    "        hidden = self.hidden_layer(combined) # compute the hidden layer\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def score_old(self, X, Y):\n",
    "        self.predictions = torch.zeros((len(Y),), dtype=torch.long)\n",
    "        self.actuals = torch.zeros((len(Y),), dtype=torch.long)\n",
    "        processes = []\n",
    "        for i in range(len(X)):\n",
    "            song = X[i]\n",
    "            genre = Y[i]\n",
    "            p = mp.Process(target=self.score_song, args=(song, genre, i))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "            if i % 100 == 0:\n",
    "                print(i, '/', len(X))\n",
    "        i = 0\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "            if i % 1000 == 0:\n",
    "                print(i, '/', len(X))\n",
    "        r2 = R2Score()\n",
    "        return r2(self.predictions, self.actuals)\n",
    "\n",
    "    def score_song(self, song, genre, i):\n",
    "        hidden = self.init_hidden()\n",
    "        song_embedding = word2vec(song.split())\n",
    "        song_tensor = torch.tensor(np.expand_dims(song_embedding, axis=1), dtype=torch.float).detach()\n",
    "        for j in range(song_tensor.size()[0]):\n",
    "            output, hidden = self.forward(song_tensor[j], hidden.detach())\n",
    "        self.predictions[i] = output.topk(1)[1]\n",
    "        self.actuals[i] = genre\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        self.predictions = torch.zeros((len(Y)))\n",
    "        self.actuals = torch.zeros((len(Y)))\n",
    "        for i in range(len(X)):\n",
    "            song = X[i]\n",
    "            genre = Y[i]\n",
    "            self.score_song(song, genre, i)\n",
    "            if i % 100 == 0:\n",
    "                print(i, '/', len(X))\n",
    "        r2 = R2Score()\n",
    "        return r2(self.predictions, self.actuals)\n",
    "\n",
    "\n",
    "def train_model_rnn(X, Y):\n",
    "    model = RNN(input_size=100, output_size=7, hidden_size=100, n_layers=2)\n",
    "\n",
    "    n_iters = 2\n",
    "    lr=1e-4\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # reshape X\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden()\n",
    "        r_i = np.random.randint(0, len(X))\n",
    "        song_embedding = word2vec(X[r_i].split())\n",
    "        if len(song_embedding) == 0:\n",
    "            continue\n",
    "        song_tensor = torch.tensor(np.expand_dims(song_embedding, axis=1), dtype=torch.float)\n",
    "        genre_tensor = torch.tensor(np.expand_dims(Y[r_i], axis=0), dtype=torch.long)\n",
    "        for i in range(song_tensor.size()[0]):\n",
    "            output, hidden = model.forward(song_tensor[i], hidden)\n",
    "        loss = criterion(output, genre_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iter % 1000 == 0:\n",
    "            print(f'{iter}/{n_iters}')\n",
    "    return model\n",
    "\n",
    "# train model\n",
    "model_rnn = train_model_rnn(X_vec_train, Y_vec_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 236193\n",
      "100 / 236193\n",
      "200 / 236193\n",
      "300 / 236193\n",
      "400 / 236193\n",
      "500 / 236193\n",
      "600 / 236193\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_model_rnn(model, X_train, Y_train, X_test, Y_test):\n",
    "    train_accuracy = model.score(X_train, Y_train)\n",
    "    print('\\nTraining:')\n",
    "    print(' accuracy:',format( 100*train_accuracy , '.2f') ) \n",
    "\n",
    "    # Compute and print accuracy on the test data\n",
    "    print('\\nTesting: ')\n",
    "    test_accuracy = model.score(X_test, Y_test)\n",
    "    print(' accuracy:', format( 100*test_accuracy , '.2f') )\n",
    "\n",
    "evaluate_model_rnn(model_rnn, X_vec_train, Y_vec_train, X_vec_test, Y_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the results of Logistic Regression on BOW, Neural Network on BOW, Deep Neural Neural Network on BOW, and RNN on Word Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42882c6df7bbab75554bca0716bce55d5aed073351b42d738d29bd30decd2e04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
